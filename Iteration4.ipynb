{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark session\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('iteration4').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import without schema\n",
    "df = spark.read.csv('dataset.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format\n",
    "from pyspark.sql.types import (StructField, StructType,\n",
    "                               TimestampType, IntegerType, FloatType)\n",
    "# define data schema (or use inferSchema =True when loading dataframe)\n",
    "data_schema = [StructField('instant', IntegerType(), True),\n",
    "               StructField('dteday', TimestampType(), True),\n",
    "               StructField('season', IntegerType(), True),\n",
    "               StructField('yr', IntegerType(), True),\n",
    "               StructField('mnth', IntegerType(), True),\n",
    "               StructField('hr', IntegerType(), True),\n",
    "               StructField('holiday', IntegerType(), True),\n",
    "               StructField('weekday', IntegerType(), True),\n",
    "               StructField('workingday', IntegerType(), True),\n",
    "               StructField('weathersit', IntegerType(), True),\n",
    "               StructField('temp', FloatType(), True),\n",
    "               StructField('atemp', FloatType(), True),\n",
    "               StructField('hum', FloatType(), True),\n",
    "               StructField('windspeed', FloatType(), True),\n",
    "               StructField('casual', IntegerType(), True),\n",
    "               StructField('registered', IntegerType(), True),\n",
    "               StructField('cnt', IntegerType(), True)]\n",
    "\n",
    "final_struct = StructType(fields = data_schema)\n",
    "#import with self-defined schema\n",
    "df = spark.read.csv('dataset.csv', schema=final_struct, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import with inferred schema automatically, only for csv\n",
    "df = spark.read.csv('dataset.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(5) #df.head()\n",
    "# check features and data type\n",
    "df.printSchema() #df.columns & df.dtypes\n",
    "# check number of records\n",
    "#print('Number of Rows: ', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant Python libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "data = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sn.boxplot(data['yr'], data['cnt'])\n",
    "plt.title('the influnce of year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.pointplot(data['mnth'], data['cnt'])\n",
    "plt.title('the influnce of month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.barplot(data['hr'], data['cnt'])\n",
    "plt.title('the influnce of hours in a day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.barplot(data['weathersit'], data['cnt'])\n",
    "plt.title('the influnce of weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----+-----+-----+\n",
      "|summary|instant| temp|atemp|  hum|  cnt|\n",
      "+-------+-------+-----+-----+-----+-----+\n",
      "|  count|  13411|16879|16655|16664|17372|\n",
      "+-------+-------+-----+-----+-----+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data audit\n",
    "df.describe('instant', 'temp', 'atemp', 'hum', 'cnt').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Select\n",
    "sn.barplot(data['weekday'], data['cnt'])\n",
    "plt.title('rental counts in each weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.barplot(data['season'], data['cnt'])\n",
    "plt.title('the influnce of season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df.drop('dteday', 'registered', 'casual', 'season', 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Clean\n",
    "# drop feature 'instant'\n",
    "df_cleaned = df_selected.drop('instant')\n",
    "# remove rows where 'cnt' is null\n",
    "df_cleaned = df_cleaned.na.drop(subset='cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.show(2)\n",
    "df_cleaned.describe('cnt').show()\n",
    "df_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values with mean of values in 'temp', 'atemp', 'hum'\n",
    "from pyspark.sql.functions import mean\n",
    "mean_temp = df.select(mean(df.temp)).collect()[0][0]\n",
    "mean_atemp = df.select(mean(df.atemp)).collect()[0][0]\n",
    "mean_hum = df.select(mean(df.hum)).collect()[0][0]\n",
    "mean = {'temp': mean_temp, 'atemp': mean_atemp, 'hum': mean_hum}\n",
    "df_cleaned = df_cleaned.na.fill(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-----+\n",
      "|summary| temp|atemp|  hum|  cnt|\n",
      "+-------+-----+-----+-----+-----+\n",
      "|  count|17372|17372|17372|17372|\n",
      "+-------+-----+-----+-----+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: double (nullable = false)\n",
      " |-- atemp: double (nullable = false)\n",
      " |-- hum: double (nullable = false)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.describe('temp', 'atemp', 'hum', 'cnt').show(1)\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Data\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "# one hot encode: convert numbers into a vector\n",
    "mnthEncoder = OneHotEncoder(inputCol='mnth', outputCol='mnthVec')\n",
    "hrEncoder = OneHotEncoder(inputCol='hr', outputCol='hrVec')\n",
    "weatherEncoder = OneHotEncoder(inputCol='weathersit', outputCol='weatherVec')\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [mnthEncoder, hrEncoder, weatherEncoder])\n",
    "df_constructed = pipeline.fit(df_cleaned).transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_constructed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the data\n",
    "df_reduced = df_constructed.drop('mnth', 'hr', 'weathersit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the data\n",
    "# assemble features into a vector for modeling\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "featuresCol = df_reduced.drop('cnt').columns\n",
    "assembler = VectorAssembler(inputCols = featuresCol, outputCol = 'features')\n",
    "df_projected = assembler.transform(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = df_projected.select('cnt', 'features')\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import (RandomForestRegressor, \n",
    "                                   GBTRegressor, \n",
    "                                   DecisionTreeRegressor)\n",
    "# create evaluator with R2\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol='cnt', predictionCol='prediction',\n",
    "                                metricName='r2')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample for model test\n",
    "sample, x = final_data.randomSplit([0.1, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models with different parameters and evaluate\n",
    "# random forest regression model with maxDepth: 3, 6, 9,..., 30\n",
    "r2_rfr = np.zeros(10)\n",
    "for i in np.arange(10):\n",
    "    rfr = RandomForestRegressor(labelCol='cnt', maxDepth=(i+1)*3)\n",
    "    rfrModel = rfr.fit(sample)\n",
    "    prediction_rfr = rfrModel.transform(sample)\n",
    "    r2_rfr[i] = evaluator.evaluate(prediction_rfr)\n",
    "plt.plot(np.arange(3, 33, 3), r2_rfr)\n",
    "r2_rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosted Trees model with maxIter: 10, 20, 30,..., 100\n",
    "r2_gbt = np.zeros(10)\n",
    "for i in np.arange(10):\n",
    "    gbt = GBTRegressor(labelCol='cnt', maxIter = (i+1)*10)\n",
    "    gbtModel = gbt.fit(sample)\n",
    "    prediction_gbt = gbtModel.transform(sample)\n",
    "    r2_gbt[i] = evaluator.evaluate(prediction_gbt)\n",
    "plt.plot(np.arange(10, 105, 10), r2_gbt)\n",
    "r2_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression model with maxDepth: 3, 6, 9,..., 30\n",
    "r2_dtr = np.zeros(10)\n",
    "for i in np.arange(10):\n",
    "    dtr = DecisionTreeRegressor(labelCol='cnt', maxDepth= (i+1)*3)\n",
    "    dtrModel = dtr.fit(sample)\n",
    "    prediction_dtr = dtrModel.transform(sample)\n",
    "    r2_dtr[i] = evaluator.evaluate(prediction_dtr)\n",
    "plt.plot(np.arange(3, 33, 3), r2_dtr)\n",
    "r2_dtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train, test = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "GBT = GBTRegressor(labelCol='cnt', maxIter = 80)\n",
    "# train the model\n",
    "GBTmodel = GBT.fit(train)\n",
    "prediction_GBT = GBTmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR = DecisionTreeRegressor(labelCol='cnt', maxDepth=20)\n",
    "DTRmodel = DTR.fit(train)\n",
    "prediction_DTR = DTRmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR = RandomForestRegressor(labelCol='cnt', maxDepth=20)\n",
    "RFRmodel = RFR.fit(train)\n",
    "prediction_RFR = RFRmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search patterns\n",
    "axes = plt.subplots(2, 1, figsize = (16,10))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "sn.pointplot(data['hr'], data['cnt'], hue=data['workingday'], ax=ax1)\n",
    "ax1.set_title('the influnce of hour in workingday')\n",
    "ax2 = plt.subplot(2, 2, 3)\n",
    "sn.pointplot(data['hr'], data['cnt'], hue=data['holiday'], ax=ax2)\n",
    "ax2.set_title('the influnce of hour in holiday')\n",
    "ax3 = plt.subplot(2, 2, 4)\n",
    "sn.pointplot(data['hr'], data['cnt'], hue=data['weekday'], ax=ax3)\n",
    "ax3.set_title('the influnce of hour in weekday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#virsualize patterns\n",
    "axes = plt.subplots(2, 1, figsize = (16,10))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "sn.pointplot(data['hr'], data['registered'], ax=ax1)\n",
    "ax1.set_title('registered rental counts in a day')\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "sn.pointplot(data['hr'], data['casual'], ax=ax2)\n",
    "ax2.set_title('casual rental counts in a day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "r2_GBT = evaluator.evaluate(prediction_GBT)\n",
    "r2_DTR = evaluator.evaluate(prediction_DTR)\n",
    "r2_RFR = evaluator.evaluate(prediction_RFR)\n",
    "print('R2 Score of GBT Regression: ', r2_GBT)\n",
    "print('R2 Score of Decision Tree Regression: ', r2_DTR)\n",
    "print('R2 Score of Random Forest Regression: ', r2_RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

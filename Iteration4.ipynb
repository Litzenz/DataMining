{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark session\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('iteration4').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import and Exploration\n",
    "    import types from pyspark.sql.type:\n",
    "        StructField('col', DataType, nullable=True)\n",
    "        StructType(fields=StructField)\n",
    "    spark.read.csv('file', schema=StructType)\n",
    "    spark.read.csv('file', inferSchema=True)\n",
    "    spark.read.format().load('file'): specify format of file\n",
    "    df.show(n): visualise data frame\n",
    "    df.head()\n",
    "    df.columns(): check columns\n",
    "    df.count(): check number of rows\n",
    "    df.describe('col').show(): summary of col\n",
    "    df.printSchema(): check data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import without schema\n",
    "df_noschema = spark.read.csv('dataset.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StructType,\n",
    "                               TimestampType, IntegerType, FloatType)\n",
    "# define data schema (or use inferSchema =True when loading dataframe)\n",
    "data_schema = [StructField('instant', IntegerType(), True),\n",
    "               StructField('dteday', TimestampType(), True),\n",
    "               StructField('season', IntegerType(), True),\n",
    "               StructField('yr', IntegerType(), True),\n",
    "               StructField('mnth', IntegerType(), True),\n",
    "               StructField('hr', IntegerType(), True),\n",
    "               StructField('holiday', IntegerType(), True),\n",
    "               StructField('weekday', IntegerType(), True),\n",
    "               StructField('workingday', IntegerType(), True),\n",
    "               StructField('weathersit', IntegerType(), True),\n",
    "               StructField('temp', FloatType(), True),\n",
    "               StructField('atemp', FloatType(), True),\n",
    "               StructField('hum', FloatType(), True),\n",
    "               StructField('windspeed', FloatType(), True),\n",
    "               StructField('casual', IntegerType(), True),\n",
    "               StructField('registered', IntegerType(), True),\n",
    "               StructField('cnt', IntegerType(), True)]\n",
    "\n",
    "final_struct = StructType(fields = data_schema)\n",
    "#import with self-defined schema\n",
    "df_withschema = spark.read.csv('dataset.csv', schema=final_struct, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import with inferred schema automatically, only for csv\n",
    "df = spark.read.csv('dataset.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|instant|  dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|casual|registered|cnt|\n",
      "+-------+--------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|   null|2011/1/1|     1|  0|   1|  0|      0|      6|         0|         1|0.24|0.2879|0.81|      0.0|     3|        13| 16|\n",
      "|      2|2011/1/1|     1|  0|   1|  1|      0|      6|         0|         1|0.22|  null| 0.8|      0.0|     8|        32| 40|\n",
      "|      3|2011/1/1|     1|  0|   1|  2|      0|      6|         0|         1|0.22|0.2727|null|      0.0|     5|        27| 32|\n",
      "|      4|2011/1/1|     1|  0|   1|  3|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     3|        10| 13|\n",
      "|      5|2011/1/1|     1|  0|   1|  4|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     0|         1|  1|\n",
      "|      6|2011/1/1|     1|  0|   1|  5|      0|      6|         0|         2|0.24|0.2576|0.75|   0.0896|     0|         1|  1|\n",
      "|      7|2011/1/1|     1|  0|   1|  6|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     2|         0|  2|\n",
      "|   null|2011/1/1|     1|  0|   1|  7|      0|      6|         0|         1| 0.2|0.2576|0.86|      0.0|     1|         2|  3|\n",
      "|      9|2011/1/1|     1|  0|   1|  8|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     1|         7|  8|\n",
      "|     10|2011/1/1|     1|  0|   1|  9|      0|      6|         0|         1|0.32|0.3485|0.76|      0.0|     8|         6| 14|\n",
      "+-------+--------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10) #df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17379"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- instant: integer (nullable = true)\n",
      " |-- dteday: string (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- casual: integer (nullable = true)\n",
      " |-- registered: integer (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check features and data type\n",
    "df.printSchema() #df.columns & df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|summary|          instant|               temp|              atemp|                hum|          windspeed|               cnt|\n",
      "+-------+-----------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|  count|            13411|              16879|              16655|              16664|              17379|             17379|\n",
      "|   mean|8742.028782342853|0.49789324012086394| 0.4771872830981645| 0.6233845415266452| 0.1900976063064631|189.46308763450142|\n",
      "| stddev|5057.902777166932|0.19198569516113423|0.17226178748927362|0.19323537329268173|0.12234022857279034| 181.3875990918646|\n",
      "|    min|                2|               0.02|                0.0|                0.0|                0.0|                 1|\n",
      "|    max|            17379|                1.0|                1.0|                1.0|             0.8507|               977|\n",
      "+-------+-----------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data summary\n",
    "df.describe('instant', 'temp', 'atemp', 'hum', 'windspeed', 'cnt').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Operation\n",
    "    df.col / df['col']: pick a column\n",
    "    df.select('col'): subset a dataframe\n",
    "    df.withColumn('col', fun): add column\n",
    "    df.withColumnRenamed('col', 'new_col'): rename col\n",
    "    df.filter(condition): filter rows\n",
    "    .collect()\n",
    "    df.groupBy('col'): group data by values in 'col'\n",
    "    df.groupBy('col1').fun('col2'): group by 'col1' then do fun on 'col2'\n",
    "    df.agg({'col':\"fun\"}): do fun on values in 'col' (not grouping)\n",
    "    df.ordrBy('col'): sort data by values in 'col' (same as df.sort())\n",
    "    df.sort('col', ascending=False) or df.sort(df.col.desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "    df.drop(subset): drop columns\n",
    "    df.na.drop(thresh, how, subset): drop rows with na\n",
    "    df.na.fill('value', subset): better to fill missing values with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dteday: string (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- casual: integer (nullable = true)\n",
      " |-- registered: integer (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Select\n",
    "df.drop('instant').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17379\n",
      "12069\n",
      "13411\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "# dorp any rows with missing data\n",
    "print(df.na.drop().count())\n",
    "# drop a row if a value from a particular row is missing\n",
    "print(df.na.drop(subset='instant').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "mean_temp = df.select(mean(df['temp'])).collect()\n",
    "mean_atemp = df.select(mean(df['atemp'])).collect()\n",
    "mean_hum = df.select(mean(df['hum'])).collect()\n",
    "mean_windspeed = df.select(mean(df['windspeed'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49789324012086394"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's a list\n",
    "type(mean_temp)\n",
    "mean_temp[0][0]\n",
    "# get the value\n",
    "mean_temp_value = mean_temp[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it again\n",
    "from pyspark.sql.functions import mean\n",
    "mean_temp = df.select(mean(df.temp)).collect()[0][0]\n",
    "mean_atemp = df.select(mean(df.atemp)).collect()[0][0]\n",
    "mean_hum = df.select(mean(df.hum)).collect()[0][0]\n",
    "mean_windspeed = df.select(mean(df.windspeed)).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|               temp|\n",
      "+-------+-------------------+\n",
      "|  count|              17379|\n",
      "|   mean|0.49789324012086117|\n",
      "| stddev|0.18920363048478592|\n",
      "|    min|               0.02|\n",
      "|    max|                1.0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean_temp, subset='temp').describe('temp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+-------------------+\n",
      "|summary|               temp|              atemp|                hum|          windspeed|\n",
      "+-------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  count|              17379|              17379|              17379|              17379|\n",
      "|   mean|0.49789324012086117| 0.4771872830981634|   0.62338454152664| 0.1900976063064631|\n",
      "| stddev|0.18920363048478592|0.16863523924975896|0.18921838489870607|0.12234022857279034|\n",
      "|    min|               0.02|                0.0|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|                1.0|             0.8507|\n",
      "+-------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we need fill 4 fields, put a dict as value\n",
    "mean_filled = {'temp': mean_temp, 'atemp': mean_atemp,\n",
    "               'hum': mean_hum, 'windspeed': mean_windspeed}\n",
    "df.na.fill(mean_filled).describe(['temp', 'atemp', 'hum', 'windspeed']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Basic\n",
    "    pyspark.ml.regression\n",
    "    spark.read.format('libsvm').load('file')\n",
    "    format libsvm: \n",
    "        the predictor in 'label' column \n",
    "        group all other features into 'feature' column as a vector\n",
    "    train, test = data.randomSplit([0.8, 0.2]): split data to train and test\n",
    "    LinearRegression(featuresCol, labelCol, predictonCol)\n",
    "        .fit(train): fit model with train\n",
    "            .coefficients & .intercept: y=intercept + coefficients * x\n",
    "            .summary.residuals\n",
    "            .summary.rootMeanSquareError & .summary.r2\n",
    "            .evaluate(test) #evaluate on test\n",
    "                .residuals, .rootMeanSquareError .r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('linear_regression_docs').getOrCreate()\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             label|            features|\n",
      "+------------------+--------------------+\n",
      "|-9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "|0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "|-4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.format('libsvm').load(\n",
    "    '../aws-instance/Datasets/sample_linear_regression_data.txt')\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             label|            features|\n",
      "+------------------+--------------------+\n",
      "|-9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "|0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "|-4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients: [0.0073350710225801715,0.8313757584337543,-0.8095307954684084,2.441191686884721,0.5191713795290003,1.1534591903547016,-0.2989124112808717,-0.5128514186201779,-0.619712827067017,0.6956151804322931]\n",
      "intercept: 0.14228558260358093\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "lr = LinearRegression(featuresCol='features', labelCol='label',\n",
    "                     predictionCol='prediction')\n",
    "# fit model\n",
    "lrModel = lr.fit(data)\n",
    "# model coefficients\n",
    "print('coefficients:', lrModel.coefficients)\n",
    "# model intercept: \n",
    "print('intercept:', lrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-11.011130022096554|\n",
      "| 0.9236590911176538|\n",
      "|-4.5957401897776675|\n",
      "|  -20.4201774575836|\n",
      "|-10.339160314788181|\n",
      "|-5.9552091439610555|\n",
      "|-10.726906349283922|\n",
      "|  2.122807193191233|\n",
      "|  4.077122222293811|\n",
      "|-17.316168071241652|\n",
      "| -4.593044343959059|\n",
      "|  6.380476690746936|\n",
      "| 11.320566035059846|\n",
      "|-20.721971774534094|\n",
      "| -2.736692773777401|\n",
      "| -16.66886934252847|\n",
      "|  8.242186378876315|\n",
      "|-1.3723486332690233|\n",
      "|-0.7060332131264666|\n",
      "|-1.1591135969994064|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# residuals: 残差\n",
    "lrModel.summary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.16309157133015"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE: RMSE评估模型拟合的值和实际之间的差异是多少 (残方差/自由度)\n",
    "lrModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027839179518600154"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R2: 在y的变异中，x的解释度 (1-残差方差/y方差)\n",
    "lrModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                400|\n",
      "|   mean| 0.3291399923498743|\n",
      "| stddev| 10.024774015042741|\n",
      "|    min|-28.046018037776633|\n",
      "|    max|  27.78383192005107|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|               label|\n",
      "+-------+--------------------+\n",
      "|  count|                 101|\n",
      "|   mean|-0.02925442594316729|\n",
      "| stddev|  11.454722231650138|\n",
      "|    min| -28.571478869743427|\n",
      "|    max|  27.111027963108548|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data to train and test:\n",
    "train, test = data.randomSplit([0.8, 0.2])\n",
    "train.describe().show()\n",
    "test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -27.10436970534127|\n",
      "|-27.362271158060707|\n",
      "|-22.232825853851345|\n",
      "|   -20.788913020274|\n",
      "|-20.613088987804062|\n",
      "|  -19.9452589145536|\n",
      "| -19.63985051911979|\n",
      "|-17.469816064890384|\n",
      "|-15.774635478810264|\n",
      "|-14.314325402727688|\n",
      "|-15.584651113964368|\n",
      "|-14.211689242428674|\n",
      "| -17.22409034531794|\n",
      "|-11.375936673441624|\n",
      "| -9.940139271838657|\n",
      "|-14.623105403748585|\n",
      "|-12.727586230350337|\n",
      "|-14.801118140302819|\n",
      "| -8.549381432729586|\n",
      "| -7.859745174734732|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol='features', labelCol='label',\n",
    "                     predictionCol='prediction')\n",
    "lrModel = lr.fit(train)\n",
    "evaluate = lrModel.evaluate(test)\n",
    "\n",
    "evaluate.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.429317546440721"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0055249410462012705"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "    creat 'label' and 'features' columns for Machine Learning\n",
    "    Note data must be cleaned before transform!!!\n",
    "    use Vector Assembler to combine features into a vector\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.ml.feature import VectorAssembler\n",
    "        VectorAssembler(inputCols, outputCols)\n",
    "            .transform(data)\n",
    "    split data to train and test:\n",
    "        train, test = data.randomSplit([0.8, 0.2)]\n",
    "    LinearRegression(featuresCol = 'feasures', )\n",
    "        .fit(train) #fit model with train\n",
    "            .transform(test) #predict with test, return predictions as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('linear_regression_adv').getOrCreate()\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat 'label' and 'features' columns for ML in Spark\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../aws-instance/Datasets/ecommerce_data.csv\",\n",
    "                    inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Avg Session Length\", \"Time on App\", \n",
    "               \"Time on Website\",'Length of Membership'],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"SexVec\" does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2805.transform.\n: java.lang.IllegalArgumentException: Field \"SexVec\" does not exist.\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:263)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$5.apply(VectorAssembler.scala:116)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$5.apply(VectorAssembler.scala:116)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:116)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:54)\n\tat sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-b6520b5020fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"SexVec\" does not exist.'"
     ]
    }
   ],
   "source": [
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Email='mstephenson@fernandez.com', Address='835 Frank TunnelWrightmouth, MI 82180-9605', Avatar='Violet', Avg Session Length=34.49726772511229, Time on App=12.65565114916675, Time on Website=39.57766801952616, Length of Membership=4.0826206329529615, Yearly Amount Spent=587.9510539684005, features=DenseVector([34.4973, 12.6557, 39.5777, 4.0826]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select('features', 'Yearly Amount Spent')\n",
    "\n",
    "train, test = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                349|\n",
      "|   mean|  497.2037850000424|\n",
      "| stddev|  82.92872644977007|\n",
      "|    min| 256.67058229005585|\n",
      "|    max|  765.5184619388373|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                151|\n",
      "|   mean|  504.1913785713948|\n",
      "| stddev|  70.27203343959737|\n",
      "|    min|  319.9288698031936|\n",
      "|    max|  744.2218671047146|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe().show()\n",
    "test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([26.2299, 39.0195, 0.5006, 61.7276])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol = 'features',\n",
    "                     labelCol = 'Yearly Amount Spent')\n",
    "lrModel = lr.fit(train)\n",
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1074.9876670937497"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we dont have predictors in test set\n",
    "# create a dataframe only with features\n",
    "unlabeled_data = test.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[30.3931845423455...|329.57744551625024|\n",
      "|[30.5743636841713...|440.19760523850437|\n",
      "|[30.7377203726281...| 449.7339383163787|\n",
      "|[30.9716756438877...| 486.4357352274369|\n",
      "|[31.1239743499119...|507.26194812921403|\n",
      "|[31.3091926408918...| 428.4087147743171|\n",
      "|[31.3123495994443...|443.51988866754914|\n",
      "|[31.4459724827577...| 481.1757144768453|\n",
      "|[31.5741380228732...| 558.0585809991367|\n",
      "|[31.6098395733896...| 426.4019150170459|\n",
      "|[31.8209982016720...| 415.8386831331484|\n",
      "|[31.8293464559211...|  382.821195515932|\n",
      "|[31.8512531286083...|463.78166546902617|\n",
      "|[31.8745516945853...|  396.586543553414|\n",
      "|[31.9048571310136...| 490.3090140905101|\n",
      "|[31.9096268275227...| 550.9888513804885|\n",
      "|[31.9262720263601...| 379.0218985348372|\n",
      "|[31.9673209478824...|  449.215243386544|\n",
      "|[32.0215955013870...| 516.0032914941355|\n",
      "|[32.0305497162129...| 588.5545772614591|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree  / Logistic Regression\n",
    "    Gradient Boosted Trees uses ensembles of decision trees\n",
    "    Evaluation is different from linear regression!!\n",
    "        forom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "        from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "        RandomForest, GBT, DecisionTree, LogisticRegression\n",
    "            model.transform(test) to get predictions as a column\n",
    "            evaluator(label_prediction) get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC曲线: sensitive 与 (1-specifity) 曲线  \n",
    "- TPR：True Positive Rate，将实际的1正确地预测为1的概率，d/(c+d)。  \n",
    "    TPR也称为Sensitivity，即“正例的覆盖率”——将实际为1的样本数找出来的概率。  \n",
    "- FPR：False Positive Rate，将实际的0错误地预测为1的概率，b/(a+b)。  \n",
    "    1-FPR其实就是“负例的覆盖率”，也就是把负例正确地识别为负例的概率  \n",
    "- AUC: ROC曲线下的面积, 可以定量地评价模型的效果, AUC越大则模型效果越好  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier,\n",
    "                                       DecisionTreeClassifier, LogisticRegression)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loda data with format 'libsvm'\n",
    "data = spark.read.format('libsvm').load(\n",
    "    '../aws-instance/Datasets/sample_libsvm_data.txt')\n",
    "data.printSchema()\n",
    "# split data into train and test\n",
    "(train, test) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "dt = DecisionTreeClassifier()\n",
    "# random Forest: numTrees\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\", numTrees=20)\n",
    "# model define: parameter maxIteration\n",
    "gbt = GBTClassifier(labelCol='label', featuresCol='features', maxIter=10)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# train models\n",
    "model_dt = dt.fit(train)\n",
    "model_rf = rf.fit(train)\n",
    "model_gb = gbt.fit(train)\n",
    "model_lr = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check logistic regression model\n",
    "model_lr.summary.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of decision tree and random forest\n",
    "# return label_prediction dataframe\n",
    "prediction_dt = model_dt.transform(test)\n",
    "prediction_rf = model_rf.transform(test)\n",
    "prediction_gb = model_gb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[123,124,125...|[32.1546438853725...|[0.99999999999998...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[21.6733611787581...|[0.99999999961329...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "prediction_lr = model_lr.evaluate(test)\n",
    "prediction_lr.predictions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[18.6964270259833...|[0.99999999240994...|       0.0|\n",
      "|  0.0|(692,[121,122,123...|[21.9449398887477...|[0.99999999970526...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lr.transform(test).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define evaluation method\n",
    "evaluatorBi = BinaryClassificationEvaluator()\n",
    "# evaluate prdiction value with label value: ROC\n",
    "evaluatorBi.evaluate(prediction_lr.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define evaluation method (accuracy)\n",
    "evaluatorMc = MulticlassClassificationEvaluator(predictionCol='prediction',\n",
    "                                               labelCol='label',\n",
    "                                               metricName='accuracy')\n",
    "evaluatorMc.evaluate(prediction_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorMc.evaluate(prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorMc.evaluate(prediction_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorMc.evaluate(prediction_lr.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Advanced\n",
    "    with categorical fetures (0, 1 do not need transform)\n",
    "    StringIndexer: assign number to each category \n",
    "    OneHotEncoder: combine numbers into a vector \n",
    "    from spark.ml.feature import (StringIndexer, OneHotEncoder)\n",
    "        a = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
    "        b = a.fit(df) ?\n",
    "        td = c.transform(df)\n",
    "    \n",
    "    from spark.ml.feature import VectorAssembler\n",
    "        a = VectorAssembler(inputCols, outputCol)\n",
    "        b = a.transform(df)\n",
    "    \n",
    "    Pipeline: list all works to do on an object\n",
    "    from spark.ml import Pipeline\n",
    "        a = Pipeline(stages)\n",
    "        b = a.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.appName('logistic_regression_adv').getOrCreate()\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "df = spark.read.csv('../aws-instance/Datasets/titanic_data.csv',\n",
    "                      inferSchema=True, header=True)\n",
    "# check type of features\n",
    "df.printSchema()\n",
    "# check number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selected = df.drop('PassengerId', 'Name', 'Ticket', 'Cabin')\n",
    "data_cleaned = data_selected.na.drop()\n",
    "data_cleaned.printSchema()\n",
    "data_cleaned.show(3)\n",
    "data_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with categorical features\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a string index: assign a number to each category in one column\n",
    "gender_index = StringIndexer(inputCol='Sex', outputCol='SexIndex')\n",
    "embark_index = StringIndexer(inputCol='Embarked', outputCol='EmbarkIndex')\n",
    "# one hot encode numbers: convert numbers into a vector\n",
    "gender_encode = OneHotEncoder(inputCol='SexIndex', outputCol='SexVec')\n",
    "embark_encode = OneHotEncoder(inputCol='EmbarkIndex', outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an vector assembler: combine all features as a vector\n",
    "assembler = VectorAssembler(inputCols = ['Pclass',\n",
    "                                        'SexVec',\n",
    "                                        'Age',\n",
    "                                        'SibSp',\n",
    "                                        'Parch',\n",
    "                                        'Fare',\n",
    "                                        'EmbarkVec'],\n",
    "                           outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "from pyspark.ml import Pipeline\n",
    "# list what to do with the data frame\n",
    "pipeline = Pipeline(stages=[gender_index, embark_index,\n",
    "                           gender_encode, embark_encode,\n",
    "                           assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train\n",
    "(train, test) = data_cleaned.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Survived',\n",
       " 'Pclass',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'Embarked',\n",
       " 'SexIndex',\n",
       " 'EmbarkIndex',\n",
       " 'SexVec',\n",
       " 'EmbarkVec',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pipeline to deal with the data frame, including:\n",
    "# gender_index, embark_index, gender_encode, embark_encode, assembler, lr\n",
    "model = pipeline.fit(train)\n",
    "# predict on test data\n",
    "results = model.transform(test)\n",
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with binary classification evaluater\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8646232439335882"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC: area under curve (ROC)\n",
    "evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Method Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('tree_methods_adv').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      "\n",
      "Abilene Christian University\n",
      "Yes\n",
      "1660\n",
      "1232\n",
      "721\n",
      "23\n",
      "52\n",
      "2885\n",
      "537\n",
      "7440\n",
      "3300\n",
      "450\n",
      "2200\n",
      "70\n",
      "78\n",
      "18.1\n",
      "12\n",
      "7041\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('../aws-instance/Datasets/college_data.csv', \n",
    "                    inferSchema=True, header=True)\n",
    "data.printSchema()\n",
    "for i in data.head():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create assembler\n",
    "colsTrans = data.drop('School', 'Private').columns\n",
    "assembler = VectorAssembler(inputCols=colsTrans, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|Private|            features|\n",
      "+-------+--------------------+\n",
      "|    Yes|[1660.0,1232.0,72...|\n",
      "|    Yes|[2186.0,1924.0,51...|\n",
      "|    Yes|[1428.0,1097.0,33...|\n",
      "+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform the data\n",
    "output = assembler.transform(data)\n",
    "output.select('Private', 'features').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category feature 'Private' need to be indexed as 0, 1\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndex = StringIndexer(inputCol='Private', outputCol='PrivateIndex')\n",
    "output_indexed = stringIndex.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|PrivateIndex|            features|\n",
      "+------------+--------------------+\n",
      "|         0.0|[1660.0,1232.0,72...|\n",
      "|         0.0|[2186.0,1924.0,51...|\n",
      "|         0.0|[1428.0,1097.0,33...|\n",
      "+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output_indexed.select('PrivateIndex', 'features')\n",
    "final_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "train, test = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "from pyspark.ml.classification import (DecisionTreeClassifier, GBTClassifier,\n",
    "                                       RandomForestClassifier)\n",
    "dtc = DecisionTreeClassifier(labelCol='PrivateIndex', featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='PrivateIndex')\n",
    "gbt = GBTClassifier(labelCol='PrivateIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "dtc_model = dtc.fit(train)\n",
    "rfc_model = rfc.fit(train)\n",
    "gbt_model = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+\n",
      "|PrivateIndex|            features|prediction|\n",
      "+------------+--------------------+----------+\n",
      "|         0.0|[141.0,118.0,55.0...|       0.0|\n",
      "|         0.0|[167.0,130.0,46.0...|       0.0|\n",
      "|         0.0|[174.0,146.0,88.0...|       0.0|\n",
      "+------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model on test set\n",
    "dtc_predictions = dtc_model.transform(test)\n",
    "rfc_predictions = rfc_model.transform(test)\n",
    "gbt_predictions = gbt_model.transform(test)\n",
    "# note that only 'prediction' colummn added in gbt_predictions\n",
    "gbt_predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# create the binary evaluator for AUC(ROC)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluatorBi = BinaryClassificationEvaluator(labelCol = 'PrivateIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9317841880341879"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorBi.evaluate(dtc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9677884615384615"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorBi.evaluate(rfc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9057692307692309"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no rawPrediction col in gbt_predictions, need specify it as prediction\n",
    "evaluatorBi_gbt = BinaryClassificationEvaluator(labelCol='PrivateIndex',\n",
    "                                               rawPredictionCol='prediction')\n",
    "evaluatorBi_gbt.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multiclass evaluator for accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluatorMc = MulticlassClassificationEvaluator(labelCol='PrivateIndex',\n",
    "                                               predictionCol='prediction',\n",
    "                                               metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9212962962962963"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorMc.evaluate(dtc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9305555555555556"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorMc.evaluate(rfc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9305555555555556"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorMc.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "    numpy: convert dataframe to array\n",
    "    matplotlib: plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant Python libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([577.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 314.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sex = np.array(df.select('Sex').collect())\n",
    "plt.hist(sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 54.,  46., 177., 169., 118.,  70.,  45.,  24.,   9.,   2.]),\n",
       " array([ 0.42 ,  8.378, 16.336, 24.294, 32.252, 40.21 , 48.168, 56.126,\n",
       "        64.084, 72.042, 80.   ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "age = np.array(df.filter(df.Age > 0).select('Age').collect())\n",
    "plt.hist(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = results.filter(results.Survived == results.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
